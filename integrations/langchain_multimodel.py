#!/usr/bin/env python3
"""
LangChain å¤šæ¨¡å‹é›†æˆ - æ”¯æŒ Gemini å’Œ Groq

ä½¿ç”¨æ–¹æ³•:
    1. pip install langchain langchain-google-genai langchain-groq python-dotenv
    2. cp .env.example .env
    3. ç¼–è¾‘ .env å¡«å…¥ä½ çš„ API Key
    4. python integrations/langchain_multimodel.py

æ”¯æŒçš„æ¨¡å‹:
    - Gemini: gemini-2.5-flash-lite, gemini-2.5-flash, gemini-2.5-pro
    - Groq: llama-3.3-70b-versatile, mixtral-8x7b-32768, gemma2-9b-it
"""

import os
import sys
from typing import Optional, Literal
from datetime import datetime
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'src'))

# è‡ªåŠ¨åŠ è½½ .env æ–‡ä»¶
try:
    from dotenv import load_dotenv
    env_path = project_root / '.env'
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… å·²ä» {env_path} åŠ è½½ç¯å¢ƒå˜é‡")
    else:
        # å°è¯•ä»å½“å‰ç›®å½•åŠ è½½
        if Path('.env').exists():
            load_dotenv('.env')
            print("âœ… å·²ä» .env åŠ è½½ç¯å¢ƒå˜é‡")
except ImportError:
    pass  # python-dotenv æœªå®‰è£…ï¼Œä¾èµ–æ‰‹åŠ¨ export

# ============================================================
# 1ï¸âƒ£ æ£€æŸ¥ä¾èµ–
# ============================================================

GEMINI_AVAILABLE = False
GROQ_AVAILABLE = False
LANGCHAIN_AVAILABLE = False

try:
    from langchain.chat_models import init_chat_model
    LANGCHAIN_AVAILABLE = True
except ImportError:
    print("âš ï¸  LangChain æœªå®‰è£…ï¼Œè¿è¡Œ: pip install langchain")

try:
    import langchain_google_genai
    GEMINI_AVAILABLE = True
except ImportError:
    print("âš ï¸  Gemini é›†æˆæœªå®‰è£…ï¼Œè¿è¡Œ: pip install langchain-google-genai")

try:
    import langchain_groq
    GROQ_AVAILABLE = True
except ImportError:
    print("âš ï¸  Groq é›†æˆæœªå®‰è£…ï¼Œè¿è¡Œ: pip install langchain-groq")

from semantic_layer import SemanticOrchestrator
from semantic_layer.models import ExecutionContext


# ============================================================
# 2ï¸âƒ£ æ¨¡å‹é…ç½®
# ============================================================

# æ¨èçš„æ¨¡å‹é…ç½®
MODEL_CONFIGS = {
    # Gemini æ¨¡å‹ï¼ˆGoogleï¼‰- é€‚åˆå¤æ‚æ¨ç†
    "gemini-flash": {
        "model": "gemini-2.5-flash-lite",
        "provider": "google_genai",
        "description": "Gemini 2.5 Flash Lite - å¿«é€Ÿå“åº”ï¼Œé€‚åˆç®€å•æŸ¥è¯¢",
        "env_key": "GOOGLE_API_KEY"
    },
    "gemini-pro": {
        "model": "gemini-2.5-pro",
        "provider": "google_genai", 
        "description": "Gemini 2.5 Pro - é«˜è´¨é‡æ¨ç†ï¼Œé€‚åˆå¤æ‚åˆ†æ",
        "env_key": "GOOGLE_API_KEY"
    },
    
    # Groq æ¨¡å‹ - è¶…å¿«æ¨ç†é€Ÿåº¦
    "groq-llama": {
        "model": "llama-3.3-70b-versatile",
        "provider": "groq",
        "description": "Llama 3.3 70B on Groq - è¶…å¿«é€Ÿåº¦ï¼Œå…è´¹é¢åº¦",
        "env_key": "GROQ_API_KEY"
    },
    "groq-mixtral": {
        "model": "mixtral-8x7b-32768",
        "provider": "groq",
        "description": "Mixtral 8x7B on Groq - å¹³è¡¡é€Ÿåº¦ä¸è´¨é‡",
        "env_key": "GROQ_API_KEY"
    },
    "groq-gemma": {
        "model": "gemma2-9b-it",
        "provider": "groq",
        "description": "Gemma 2 9B on Groq - è½»é‡çº§ï¼Œé€‚åˆç®€å•ä»»åŠ¡",
        "env_key": "GROQ_API_KEY"
    },
}


def get_available_models() -> list[str]:
    """è·å–å½“å‰å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    available = []
    for name, config in MODEL_CONFIGS.items():
        env_key = config["env_key"]
        if os.environ.get(env_key):
            if config["provider"] == "google_genai" and GEMINI_AVAILABLE:
                available.append(name)
            elif config["provider"] == "groq" and GROQ_AVAILABLE:
                available.append(name)
    return available


def create_model(model_name: str = "groq-llama", temperature: float = 0):
    """
    åˆ›å»ºæŒ‡å®šçš„æ¨¡å‹å®ä¾‹
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼ˆè§ MODEL_CONFIGSï¼‰
        temperature: æ¸©åº¦å‚æ•°
    
    Returns:
        LangChain ChatModel å®ä¾‹
    """
    if not LANGCHAIN_AVAILABLE:
        raise ImportError("è¯·å®‰è£… LangChain: pip install langchain")
    
    if model_name not in MODEL_CONFIGS:
        raise ValueError(f"æœªçŸ¥æ¨¡å‹: {model_name}ï¼Œå¯ç”¨: {list(MODEL_CONFIGS.keys())}")
    
    config = MODEL_CONFIGS[model_name]
    
    # æ£€æŸ¥ API Key
    env_key = config["env_key"]
    if not os.environ.get(env_key):
        raise ValueError(f"è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export {env_key}=your_key")
    
    # ä½¿ç”¨ init_chat_model åˆ›å»ºæ¨¡å‹
    return init_chat_model(
        model=config["model"],
        model_provider=config["provider"],
        temperature=temperature,
    )


def create_configurable_model(default_model: str = "groq-llama"):
    """
    åˆ›å»ºå¯é…ç½®çš„æ¨¡å‹ï¼ˆè¿è¡Œæ—¶å¯åˆ‡æ¢ï¼‰
    
    ä½¿ç”¨ç¤ºä¾‹:
        model = create_configurable_model()
        
        # ä½¿ç”¨ Groq
        model.invoke("Hello", config={"configurable": {"model": "llama-3.3-70b-versatile", "model_provider": "groq"}})
        
        # åˆ‡æ¢åˆ° Gemini
        model.invoke("Hello", config={"configurable": {"model": "gemini-2.5-flash-lite", "model_provider": "google_genai"}})
    """
    if not LANGCHAIN_AVAILABLE:
        raise ImportError("è¯·å®‰è£… LangChain: pip install langchain")
    
    config = MODEL_CONFIGS.get(default_model, MODEL_CONFIGS["groq-llama"])
    
    return init_chat_model(
        model=config["model"],
        model_provider=config["provider"],
        temperature=0,
        configurable_fields=("model", "model_provider", "temperature"),
    )


# ============================================================
# 3ï¸âƒ£ è¯­ä¹‰æ§åˆ¶é¢ Tool é›†æˆ
# ============================================================

_orchestrator: Optional[SemanticOrchestrator] = None

def get_orchestrator(db_path: str = "data/semantic_layer.db") -> SemanticOrchestrator:
    """è·å–æˆ–åˆ›å»º Orchestrator å•ä¾‹"""
    global _orchestrator
    
    if _orchestrator is None:
        if not os.path.exists(db_path):
            import sqlite3
            os.makedirs('data', exist_ok=True)
            conn = sqlite3.connect(db_path)
            root = os.path.dirname(os.path.dirname(__file__))
            with open(os.path.join(root, 'schema.sql'), 'r') as f:
                conn.executescript(f.read())
            with open(os.path.join(root, 'seed_data.sql'), 'r') as f:
                conn.executescript(f.read())
            conn.close()
            print("âœ… æ•°æ®åº“å·²è‡ªåŠ¨åˆå§‹åŒ–")
        _orchestrator = SemanticOrchestrator(db_path)
    return _orchestrator


def semantic_query_func(
    question: str,
    department: Optional[str] = None,
    region: Optional[str] = None,
    period: Optional[str] = None,
    line: Optional[str] = None,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
) -> str:
    """æ‰§è¡Œè¯­ä¹‰æŸ¥è¯¢çš„æ ¸å¿ƒå‡½æ•°"""
    orchestrator = get_orchestrator()
    
    parameters = {}
    if region:
        parameters['region'] = region
    if period:
        parameters['period'] = period
    if department:
        parameters['scenario'] = {'department': department}
    if line:
        parameters['line'] = line
    if start_date:
        parameters['start_date'] = start_date
    if end_date:
        parameters['end_date'] = end_date
    
    context = ExecutionContext(
        user_id=1,
        role=f'{department}_manager' if department else 'operator',
        parameters=parameters,
        timestamp=datetime.now()
    )
    
    result = orchestrator.query(
        question=question,
        parameters=parameters,
        context=context
    )
    
    if result.get('status') == 'success':
        data = result.get('data', [])
        version = result.get('version_name', 'unknown')
        audit_id = result.get('audit_id', 'N/A')
        
        if data and len(data) > 0:
            for key, value in data[0].items():
                if isinstance(value, (int, float)):
                    if 'margin' in key.lower():
                        return f"æ¯›åˆ©ç‡: {value * 100:.1f}% (ç‰ˆæœ¬: {version}, Audit: {audit_id})"
                    elif 'fpy' in key.lower():
                        return f"ä¸€æ¬¡åˆæ ¼ç‡: {value * 100:.2f}% (ç‰ˆæœ¬: {version}, Audit: {audit_id})"
                    else:
                        return f"ç»“æœ: {value:.4f} (ç‰ˆæœ¬: {version}, Audit: {audit_id})"
        return f"æŸ¥è¯¢æˆåŠŸï¼Œæ— æ•°æ®ã€‚ç‰ˆæœ¬: {version}"
    return f"æŸ¥è¯¢å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"


def create_semantic_tools():
    """åˆ›å»ºè¯­ä¹‰æ§åˆ¶é¢å·¥å…·åˆ—è¡¨"""
    try:
        from langchain.tools import tool
        from pydantic import BaseModel, Field
    except ImportError:
        print("âš ï¸  è¯·å®‰è£…: pip install langchain pydantic")
        return []
    
    class SemanticQueryInput(BaseModel):
        """è¯­ä¹‰æŸ¥è¯¢è¾“å…¥"""
        question: str = Field(description="ä¸šåŠ¡é—®é¢˜ï¼Œå¦‚'ä¸Šæœˆåä¸œåŒºæ¯›åˆ©ç‡æ˜¯å¤šå°‘ï¼Ÿ'")
        department: Optional[Literal["finance", "sales"]] = Field(
            default=None, description="éƒ¨é—¨ï¼šfinance=è´¢åŠ¡å£å¾„ï¼Œsales=é”€å”®å£å¾„"
        )
        region: Optional[str] = Field(default=None, description="åŒºåŸŸï¼Œå¦‚'åä¸œ'")
        period: Optional[str] = Field(default=None, description="æ—¶é—´å‘¨æœŸï¼Œå¦‚'2026-01'")
        line: Optional[str] = Field(default=None, description="äº§çº¿ï¼Œå¦‚'A'")
        start_date: Optional[str] = Field(default=None, description="å¼€å§‹æ—¥æœŸ")
        end_date: Optional[str] = Field(default=None, description="ç»“æŸæ—¥æœŸ")
    
    @tool("semantic_query", args_schema=SemanticQueryInput)
    def semantic_query(
        question: str,
        department: Optional[str] = None,
        region: Optional[str] = None,
        period: Optional[str] = None,
        line: Optional[str] = None,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
    ) -> str:
        """æŸ¥è¯¢ä¼ä¸šä¸šåŠ¡æŒ‡æ ‡ã€‚æ ¹æ®éƒ¨é—¨è‡ªåŠ¨é€‰æ‹©è®¡ç®—å£å¾„ï¼Œè¿”å›å¯è¿½æº¯çš„ç»“æœã€‚"""
        return semantic_query_func(question, department, region, period, line, start_date, end_date)
    
    @tool("list_metrics")
    def list_metrics() -> str:
        """åˆ—å‡ºç³»ç»Ÿæ”¯æŒçš„æ‰€æœ‰ä¸šåŠ¡æŒ‡æ ‡ã€‚"""
        orchestrator = get_orchestrator()
        objects = orchestrator.list_semantic_objects()
        result = "å¯ç”¨æŒ‡æ ‡ï¼š\n"
        for obj in objects:
            result += f"- {obj['name']}: {obj.get('description', 'N/A')[:50]}...\n"
        return result
    
    return [semantic_query, list_metrics]


def create_agent_with_tools(model_name: str = "groq-llama"):
    """
    åˆ›å»ºå¸¦è¯­ä¹‰æ§åˆ¶é¢èƒ½åŠ›çš„ Agent
    
    Args:
        model_name: ä½¿ç”¨çš„æ¨¡å‹ï¼ˆè§ MODEL_CONFIGSï¼‰
    
    Returns:
        AgentExecutor
    """
    if not LANGCHAIN_AVAILABLE:
        raise ImportError("è¯·å®‰è£…: pip install langchain")
    
    try:
        from langchain.agents import create_openai_functions_agent, AgentExecutor
        from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
    except ImportError:
        raise ImportError("è¯·å®‰è£…: pip install langchain")
    
    model = create_model(model_name, temperature=0)
    tools = create_semantic_tools()
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¼ä¸šæ•°æ®åˆ†æåŠ©æ‰‹ã€‚ä½¿ç”¨ semantic_query å·¥å…·æŸ¥è¯¢ä¸šåŠ¡æŒ‡æ ‡ã€‚

é‡è¦æç¤ºï¼š
- è´¢åŠ¡éƒ¨å’Œé”€å”®éƒ¨æŸ¥è¯¢æ¯›åˆ©ç‡ä¼šå¾—åˆ°ä¸åŒç»“æœï¼ˆå£å¾„ä¸åŒï¼‰
- æ¯ä¸ªæŸ¥è¯¢éƒ½æœ‰ Audit ID å¯è¿½æº¯
- å¦‚æœç”¨æˆ·é—®"ä¸ºä»€ä¹ˆç»“æœä¸ä¸€æ ·"ï¼Œæ£€æŸ¥éƒ¨é—¨å£å¾„å·®å¼‚
"""),
        MessagesPlaceholder(variable_name="chat_history", optional=True),
        ("human", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])
    
    # æ³¨æ„ï¼šGroq å’Œ Gemini éƒ½æ”¯æŒ OpenAI å…¼å®¹çš„ function calling
    agent = create_openai_functions_agent(model, tools, prompt)
    
    return AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=True,
        handle_parsing_errors=True
    )


# ============================================================
# 4ï¸âƒ£ æ¼”ç¤º
# ============================================================

def demo_direct_query():
    """ç›´æ¥ä½¿ç”¨è¯­ä¹‰æ§åˆ¶é¢æŸ¥è¯¢ï¼ˆæ— éœ€ LLMï¼‰"""
    print("=" * 80)
    print("ğŸ”§ ç›´æ¥è°ƒç”¨è¯­ä¹‰æ§åˆ¶é¢ï¼ˆæ— éœ€ LLM API Keyï¼‰")
    print("=" * 80)
    
    os.chdir(os.path.dirname(os.path.dirname(__file__)))
    
    print("\nğŸ’° è´¢åŠ¡è§†è§’æŸ¥è¯¢æ¯›åˆ©ç‡:")
    result = semantic_query_func(
        question="ä¸Šæœˆåä¸œåŒºæ¯›åˆ©ç‡æ˜¯å¤šå°‘ï¼Ÿ",
        department="finance",
        region="åä¸œ",
        period="2026-01"
    )
    print(f"   {result}")
    
    print("\nğŸ“ˆ é”€å”®è§†è§’æŸ¥è¯¢æ¯›åˆ©ç‡:")
    result = semantic_query_func(
        question="ä¸Šæœˆåä¸œåŒºæ¯›åˆ©ç‡æ˜¯å¤šå°‘ï¼Ÿ",
        department="sales",
        region="åä¸œ",
        period="2026-01"
    )
    print(f"   {result}")
    
    print("\nğŸ­ æŸ¥è¯¢ä¸€æ¬¡åˆæ ¼ç‡:")
    result = semantic_query_func(
        question="æ˜¨å¤©äº§çº¿Açš„ä¸€æ¬¡åˆæ ¼ç‡æ˜¯å¤šå°‘ï¼Ÿ",
        line="A",
        start_date="2026-01-27",
        end_date="2026-01-27"
    )
    print(f"   {result}")


def demo_with_llm():
    """ä½¿ç”¨ LLM Agent è¿›è¡Œå¯¹è¯"""
    print("\n" + "=" * 80)
    print("ğŸ¤– ä½¿ç”¨ LLM Agentï¼ˆéœ€è¦ API Keyï¼‰")
    print("=" * 80)
    
    # æ£€æŸ¥å¯ç”¨æ¨¡å‹
    available = get_available_models()
    if not available:
        print("\nâš ï¸  æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹ API Key")
        print("\nè¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ä¹‹ä¸€ï¼š")
        print("   export GOOGLE_API_KEY=your_gemini_key")
        print("   export GROQ_API_KEY=your_groq_key")
        print("\næ¨èä½¿ç”¨ Groqï¼ˆå…è´¹é¢åº¦ï¼Œé€Ÿåº¦æå¿«ï¼‰ï¼š")
        print("   1. è®¿é—® https://console.groq.com/keys")
        print("   2. åˆ›å»º API Key")
        print("   3. export GROQ_API_KEY=your_key")
        return
    
    print(f"\nâœ… å¯ç”¨æ¨¡å‹: {available}")
    
    # ä¼˜å…ˆä½¿ç”¨ Groqï¼ˆå…è´¹ä¸”å¿«ï¼‰
    model_name = "groq-llama" if "groq-llama" in available else available[0]
    print(f"ğŸ“Œ ä½¿ç”¨æ¨¡å‹: {model_name}")
    
    try:
        agent = create_agent_with_tools(model_name)
        
        queries = [
            "å¸®æˆ‘æŸ¥ä¸€ä¸‹ä¸Šæœˆåä¸œåŒºçš„æ¯›åˆ©ç‡ï¼Œæˆ‘æ˜¯è´¢åŠ¡éƒ¨çš„",
            "ç³»ç»Ÿæ”¯æŒæŸ¥è¯¢å“ªäº›æŒ‡æ ‡ï¼Ÿ",
        ]
        
        for query in queries:
            print(f"\nğŸ“ ç”¨æˆ·: {query}")
            print("-" * 60)
            result = agent.invoke({"input": query})
            print(f"ğŸ¤– Agent: {result['output']}")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")


def show_setup_guide():
    """æ˜¾ç¤ºé…ç½®æŒ‡å—"""
    print("\n" + "=" * 80)
    print("ğŸ“‹ é…ç½®æŒ‡å—")
    print("=" * 80)
    print("""
1. å®‰è£…ä¾èµ–ï¼š
   pip install langchain langchain-google-genai langchain-groq pydantic

2. è®¾ç½® API Keyï¼ˆäºŒé€‰ä¸€ï¼‰ï¼š

   ã€æ¨èã€‘Groqï¼ˆå…è´¹ + è¶…å¿«ï¼‰ï¼š
   - è®¿é—® https://console.groq.com/keys è·å– Key
   - export GROQ_API_KEY=gsk_xxx
   
   ã€å¯é€‰ã€‘Geminiï¼š
   - è®¿é—® https://aistudio.google.com/apikey è·å– Key
   - export GOOGLE_API_KEY=AIza_xxx

3. è¿è¡Œæµ‹è¯•ï¼š
   python integrations/langchain_multimodel.py

4. åœ¨ä»£ç ä¸­ä½¿ç”¨ï¼š

   from integrations.langchain_multimodel import create_agent_with_tools
   
   # ä½¿ç”¨ Groqï¼ˆæ¨èï¼Œå…è´¹ä¸”å¿«ï¼‰
   agent = create_agent_with_tools("groq-llama")
   result = agent.invoke({"input": "ä¸Šæœˆåä¸œåŒºæ¯›åˆ©ç‡æ˜¯å¤šå°‘ï¼Ÿæˆ‘æ˜¯è´¢åŠ¡éƒ¨çš„"})
   
   # æˆ–ä½¿ç”¨ Gemini
   agent = create_agent_with_tools("gemini-flash")
""")


if __name__ == "__main__":
    show_setup_guide()
    demo_direct_query()
    demo_with_llm()
